{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishaqmarashy/Digit-Recognizer-ML/blob/main/Digit_Recognizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "C1aZL2iT4UIU"
      },
      "outputs": [],
      "source": [
        "# Common imports\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Plotting\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Kaggle Data https://www.kaggle.com/competitions/digit-recognizer/overview\n",
        "# From CSV to Pandas DataFrame\n",
        "train = pd.read_csv( \"./train.csv\")\n",
        "test = pd.read_csv(\"./test.csv\")"
      ],
      "metadata": {
        "id": "5shLl_V7vtx-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a labels and some of the samples to see what the dataset looks like and how many samples we are working with\n",
        "print(train.info(),train.columns,\"\\n\",train['label'].value_counts(),\"\\n\",train['pixel0']) "
      ],
      "metadata": {
        "id": "qiFspa3ESDpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6321c195-9840-45a3-a0cb-1c1c282c1e21"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42000 entries, 0 to 41999\n",
            "Columns: 785 entries, label to pixel783\n",
            "dtypes: int64(785)\n",
            "memory usage: 251.5 MB\n",
            "None Index(['label', 'pixel0', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5',\n",
            "       'pixel6', 'pixel7', 'pixel8',\n",
            "       ...\n",
            "       'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779',\n",
            "       'pixel780', 'pixel781', 'pixel782', 'pixel783'],\n",
            "      dtype='object', length=785) \n",
            " 1    4684\n",
            "7    4401\n",
            "3    4351\n",
            "9    4188\n",
            "2    4177\n",
            "6    4137\n",
            "0    4132\n",
            "4    4072\n",
            "8    4063\n",
            "5    3795\n",
            "Name: label, dtype: int64 \n",
            " 0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "41995    0\n",
            "41996    0\n",
            "41997    0\n",
            "41998    0\n",
            "41999    0\n",
            "Name: pixel0, Length: 42000, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts class vectors to binary class matrix\n",
        "# more at https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\n",
        "from keras.utils.np_utils import to_categorical \n",
        "\n",
        "# The label column contains the images identity which we would like to predict\n",
        "# So we seperate it and place it in y to train our model later\n",
        "y = to_categorical(train['label'], num_classes = 10)\n",
        "# Removing the label data leaves us with the pixel information. \n",
        "X = train.drop('label',axis=1)\n",
        "# There are 784 columns(1D) thus the images need to be reshaped into 28x28(2D) Data \n",
        "X = X.values.reshape(-1,28,28,1)\n",
        "test = test.values.reshape(-1,28,28,1)\n",
        "plt.imshow(X[0][:,:,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "s7JGETqsVVGD",
        "outputId": "b50dd85c-8a44-4530-9837-cbc904a8da3a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2c930ddb20>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# z = (x – μ) / σ apply z-score normalization for faster learning\n",
        "mu = X.mean().astype(np.float32)\n",
        "std = X.std().astype(np.float32)\n",
        "X=(X-mu)/std\n",
        "test=(test-mu)/std"
      ],
      "metadata": {
        "id": "NBIAwsWH9y1H"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split our data into a training section and a seperate validation section to insure we can check for accuracy\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=42)"
      ],
      "metadata": {
        "id": "TuPIj_nFJS8X"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "# Build a neural network model to make predictions\n",
        "model = Sequential([\n",
        "        # Conv2d is a convolutional neural network(CNN) layer\n",
        "        # stride is the sliding window for each dimension of input \n",
        "        # https://keras.io/api/layers/convolution_layers/convolution2d/\n",
        "        # neuron count, kernal size\n",
        "        Conv2D(28,(3,3), activation='relu',input_shape=(28,28,1)),\n",
        "        # Normalize the output of the Conv2d layer above for faster training\n",
        "        # https://keras.io/api/layers/normalization_layers/batch_normalization/\n",
        "        BatchNormalization(),\n",
        "        Conv2D(28,(6,6), activation='relu'),\n",
        "        # Pools the outputs of previous layer\n",
        "        # https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D\n",
        "        MaxPool2D(),\n",
        "        # flattens the tensor output of the previous layer into a 1 dimentional tensor\n",
        "        # https://pytorch.org/docs/stable/generated/torch.flatten.html\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        # 10 specified the neuron count which should be equal to the number of labels we have\n",
        "        # softmax is a common activation function used for multi-classification problems\n",
        "        Dense(10, activation='softmax')\n",
        "        ])\n",
        "# Adam implements a variable learning rate among other features that speed up convergence\n",
        "# categorical_crossentropy is the partial derivative of the softmax function which creates a single global minima for the gradient to converge on\n",
        "# accuracy is the metric we would like to see throught he epocs cycle\n",
        "# https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#AdThrive_Content_1_desktop\n",
        "model.compile(Adam(), loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "6sEKhSsQWx3K"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Creates additional images for the model to train on by using data augmentation techniques\n",
        "datagen = ImageDataGenerator(\n",
        "    # Center images so its padding is equal from all sides\n",
        "    featurewise_center=True,\n",
        "    samplewise_center=False, \n",
        "    featurewise_std_normalization=False,\n",
        "    samplewise_std_normalization=False,\n",
        "    zca_whitening=False,\n",
        "    # Rotate images randomly by 20%\n",
        "    rotation_range=20,\n",
        "    # Shift images randomly by 10% shrinking or stretching vertically and horizontally\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    # Shift images randomly by 10% shrinking or stretching vertically and horizontally\n",
        "    horizontal_flip=False,  \n",
        "    vertical_flip=False)\n",
        "# Feed data into the data generator so it may create augmented data for us\n",
        "datagen.fit(X_train)\n",
        "# Number of iterations of the the dataset\n",
        "epochs=30\n",
        "batch_size = 32\n",
        "# Finally generate images fromt he traing examples from the learned augmentation techniques\n",
        "train_data=datagen.flow(X_train,y_train, batch_size=batch_size)\n",
        "validation_data=datagen.flow(X_train, y_train, batch_size=8)\n",
        "# Train the model using the expanded dataset \n",
        "histcb=model.fit(train_data, validation_data=validation_data,verbose = 2,steps_per_epoch=len(X_train) / batch_size, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEcA7htHGngj",
        "outputId": "84e5f26d-fee9-497a-b721-31fa17c190f4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "918/918 - 201s - loss: 0.3547 - accuracy: 0.8929 - val_loss: 0.1507 - val_accuracy: 0.9503 - 201s/epoch - 219ms/step\n",
            "Epoch 2/30\n",
            "918/918 - 225s - loss: 0.1272 - accuracy: 0.9597 - val_loss: 0.0905 - val_accuracy: 0.9723 - 225s/epoch - 245ms/step\n",
            "Epoch 3/30\n",
            "918/918 - 182s - loss: 0.0995 - accuracy: 0.9698 - val_loss: 0.1014 - val_accuracy: 0.9688 - 182s/epoch - 198ms/step\n",
            "Epoch 4/30\n",
            "918/918 - 182s - loss: 0.0894 - accuracy: 0.9735 - val_loss: 0.0801 - val_accuracy: 0.9749 - 182s/epoch - 198ms/step\n",
            "Epoch 5/30\n",
            "918/918 - 186s - loss: 0.0844 - accuracy: 0.9744 - val_loss: 0.0683 - val_accuracy: 0.9788 - 186s/epoch - 202ms/step\n",
            "Epoch 6/30\n",
            "918/918 - 183s - loss: 0.0718 - accuracy: 0.9789 - val_loss: 0.0694 - val_accuracy: 0.9797 - 183s/epoch - 199ms/step\n",
            "Epoch 7/30\n",
            "918/918 - 182s - loss: 0.0755 - accuracy: 0.9765 - val_loss: 0.0808 - val_accuracy: 0.9765 - 182s/epoch - 198ms/step\n",
            "Epoch 8/30\n",
            "918/918 - 181s - loss: 0.0663 - accuracy: 0.9803 - val_loss: 0.0655 - val_accuracy: 0.9800 - 181s/epoch - 197ms/step\n",
            "Epoch 9/30\n",
            "918/918 - 183s - loss: 0.0639 - accuracy: 0.9812 - val_loss: 0.0618 - val_accuracy: 0.9813 - 183s/epoch - 199ms/step\n",
            "Epoch 10/30\n",
            "918/918 - 182s - loss: 0.0592 - accuracy: 0.9827 - val_loss: 0.0545 - val_accuracy: 0.9837 - 182s/epoch - 198ms/step\n",
            "Epoch 11/30\n",
            "918/918 - 183s - loss: 0.0591 - accuracy: 0.9820 - val_loss: 0.0747 - val_accuracy: 0.9775 - 183s/epoch - 199ms/step\n",
            "Epoch 12/30\n",
            "918/918 - 180s - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9856 - 180s/epoch - 196ms/step\n",
            "Epoch 13/30\n",
            "918/918 - 181s - loss: 0.0550 - accuracy: 0.9834 - val_loss: 0.0538 - val_accuracy: 0.9843 - 181s/epoch - 197ms/step\n",
            "Epoch 14/30\n",
            "918/918 - 180s - loss: 0.0509 - accuracy: 0.9854 - val_loss: 0.0491 - val_accuracy: 0.9857 - 180s/epoch - 195ms/step\n",
            "Epoch 15/30\n",
            "918/918 - 182s - loss: 0.0491 - accuracy: 0.9853 - val_loss: 0.0521 - val_accuracy: 0.9838 - 182s/epoch - 198ms/step\n",
            "Epoch 16/30\n",
            "918/918 - 179s - loss: 0.0473 - accuracy: 0.9853 - val_loss: 0.0374 - val_accuracy: 0.9894 - 179s/epoch - 195ms/step\n",
            "Epoch 17/30\n",
            "918/918 - 180s - loss: 0.0467 - accuracy: 0.9857 - val_loss: 0.0410 - val_accuracy: 0.9884 - 180s/epoch - 196ms/step\n",
            "Epoch 18/30\n",
            "918/918 - 180s - loss: 0.0424 - accuracy: 0.9878 - val_loss: 0.0372 - val_accuracy: 0.9883 - 180s/epoch - 196ms/step\n",
            "Epoch 19/30\n",
            "918/918 - 182s - loss: 0.0457 - accuracy: 0.9859 - val_loss: 0.0553 - val_accuracy: 0.9846 - 182s/epoch - 198ms/step\n",
            "Epoch 20/30\n",
            "918/918 - 181s - loss: 0.0417 - accuracy: 0.9884 - val_loss: 0.0433 - val_accuracy: 0.9873 - 181s/epoch - 197ms/step\n",
            "Epoch 21/30\n",
            "918/918 - 181s - loss: 0.0429 - accuracy: 0.9866 - val_loss: 0.0474 - val_accuracy: 0.9862 - 181s/epoch - 197ms/step\n",
            "Epoch 22/30\n",
            "918/918 - 180s - loss: 0.0380 - accuracy: 0.9883 - val_loss: 0.0385 - val_accuracy: 0.9879 - 180s/epoch - 196ms/step\n",
            "Epoch 23/30\n",
            "918/918 - 181s - loss: 0.0369 - accuracy: 0.9887 - val_loss: 0.0349 - val_accuracy: 0.9895 - 181s/epoch - 197ms/step\n",
            "Epoch 24/30\n",
            "918/918 - 180s - loss: 0.0389 - accuracy: 0.9878 - val_loss: 0.0307 - val_accuracy: 0.9901 - 180s/epoch - 196ms/step\n",
            "Epoch 25/30\n",
            "918/918 - 180s - loss: 0.0346 - accuracy: 0.9892 - val_loss: 0.0356 - val_accuracy: 0.9889 - 180s/epoch - 196ms/step\n",
            "Epoch 26/30\n",
            "918/918 - 181s - loss: 0.0350 - accuracy: 0.9890 - val_loss: 0.0339 - val_accuracy: 0.9909 - 181s/epoch - 197ms/step\n",
            "Epoch 27/30\n",
            "918/918 - 181s - loss: 0.0352 - accuracy: 0.9895 - val_loss: 0.0354 - val_accuracy: 0.9896 - 181s/epoch - 197ms/step\n",
            "Epoch 28/30\n",
            "918/918 - 181s - loss: 0.0329 - accuracy: 0.9904 - val_loss: 0.0288 - val_accuracy: 0.9914 - 181s/epoch - 197ms/step\n",
            "Epoch 29/30\n",
            "918/918 - 181s - loss: 0.0362 - accuracy: 0.9893 - val_loss: 0.0378 - val_accuracy: 0.9892 - 181s/epoch - 197ms/step\n",
            "Epoch 30/30\n",
            "918/918 - 181s - loss: 0.0304 - accuracy: 0.9907 - val_loss: 0.0329 - val_accuracy: 0.9907 - 181s/epoch - 197ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions as to what the label of the image may be\n",
        "y_pred=model.predict( X_train, batch_size=batch_size, verbose=0)\n",
        "# Take probability of those predictions and pick the most probable label using argmax\n",
        "y_pred=np.argmax(y_pred,axis = 1) \n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XOWqzycNDok",
        "outputId": "c617cb6f-abb2-4d53-908e-c1a0996405e8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 0, 9, ..., 2, 6, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GC6hFZpqn5GP"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}